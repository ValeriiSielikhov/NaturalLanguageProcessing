{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acting-arkansas",
   "metadata": {},
   "source": [
    "# Transformer, Self-Attention и моделирование языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "latter-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "import dlnlputils\n",
    "from dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n",
    "    save_texts_to_file, LanguageModelDataset, load_war_and_piece_chunks, \\\n",
    "    GreedyGenerator, BeamGenerator\n",
    "from dlnlputils.pipeline import train_eval_loop, init_random_seed\n",
    "from dlnlputils.base import get_params_number\n",
    "\n",
    "import youtokentome as yttm\n",
    "\n",
    "init_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-doubt",
   "metadata": {},
   "source": [
    "## Загрузка текстов и разбиение на обучающую и тестовую подвыборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = load_war_and_piece_chunks('../datasets/war_and_peace.txt')\n",
    "len(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_chunks[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(all_chunks)\n",
    "\n",
    "TRAIN_SPLIT = int(len(all_chunks) * 0.7)\n",
    "train_texts = all_chunks[:TRAIN_SPLIT]\n",
    "test_texts = all_chunks[TRAIN_SPLIT:]\n",
    "\n",
    "print('Размер обучающей выборки', len(train_texts))\n",
    "print('Размер валидационной выборки', len(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-joining",
   "metadata": {},
   "source": [
    "## Токенизация корпуса с помощью BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-combining",
   "metadata": {},
   "source": [
    "BPE - Byte Pair Encoding\n",
    "\n",
    "YouTokenToMe - быстрая реализация BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE_MODEL_FILENAME = './models/war_and_peace_bpe.yttm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEXTS_FILENAME = './datasets/war_and_peace_bpe_train.txt'\n",
    "save_texts_to_file(train_texts, TRAIN_TEXTS_FILENAME)\n",
    "yttm.BPE.train(data=TRAIN_TEXTS_FILENAME, vocab_size=1000, model=BPE_MODEL_FILENAME);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = yttm.BPE(BPE_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(tokenizer.vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(train_texts[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_ids = tokenizer.encode(train_texts, bos=True, eos=True)\n",
    "test_token_ids = tokenizer.encode(test_texts, bos=True, eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(sent) for sent in train_token_ids], bins=30)\n",
    "plt.title('Распределение длин фрагментов в токенах')\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = np.bincount([token_id for text in train_token_ids for token_id in text])\n",
    "plt.hist(token_counts, bins=100)\n",
    "plt.title('Распределение количества упоминаний токенов')\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_subwords_in_test = sum(1 for text in test_token_ids for token_id in text if token_id == 1)\n",
    "print('Количество случаев с неизвестными n-граммами символов в валидационной выборке',\n",
    "      unknown_subwords_in_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-advancement",
   "metadata": {},
   "source": [
    "## Подготовка датасетов для PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_LENGTH = 80\n",
    "\n",
    "train_dataset = LanguageModelDataset(train_token_ids,\n",
    "                                     chunk_length=CHUNK_LENGTH)\n",
    "test_dataset = LanguageModelDataset(test_token_ids,\n",
    "                                    chunk_length=CHUNK_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(list(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-groove",
   "metadata": {},
   "source": [
    "## Общие классы и функции"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-contrary",
   "metadata": {},
   "source": [
    "### Маска зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target_dependency_mask(length):\n",
    "    full_mask = torch.ones(length, length)\n",
    "    ignore_mask = torch.tril(full_mask) < 1\n",
    "    full_mask.masked_fill_(ignore_mask, float('-inf'))\n",
    "    full_mask.masked_fill_(~ignore_mask, 0)\n",
    "    return full_mask\n",
    "\n",
    "make_target_dependency_mask(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-standard",
   "metadata": {},
   "source": [
    "### Кодирование позиции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_positional_encoding(max_length, embedding_size):\n",
    "    time = np.pi * torch.arange(0, max_length).float()\n",
    "    freq_dividers = torch.arange(1, embedding_size // 2 + 1).float()\n",
    "    inputs = time[:, None] / freq_dividers[None, :]\n",
    "    \n",
    "    result = torch.zeros(max_length, embedding_size)\n",
    "    result[:, 0::2] = torch.sin(inputs)\n",
    "    result[:, 1::2] = torch.cos(inputs)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pos_codes = make_positional_encoding(30, 30)\n",
    "plt.plot(sample_pos_codes[:, ::3].numpy());\n",
    "plt.gcf().set_size_inches((15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-december",
   "metadata": {},
   "source": [
    "### Основной класс - языковая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, backbone, emb_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "        self.backbone = backbone\n",
    "        self.out = nn.Linear(embedding_size, vocab_size)\n",
    "    \n",
    "    def forward(self, seed_token_ids):\n",
    "        \"\"\"\n",
    "            seed_token_ids - BatchSize x MaxInLen\n",
    "        \"\"\"\n",
    "        batch_size, max_in_length = seed_token_ids.shape\n",
    "\n",
    "        seed_padding_mask = seed_token_ids == 0\n",
    "        dependency_mask = make_target_dependency_mask(max_in_length) \\\n",
    "            .to(seed_token_ids.device)\n",
    "        \n",
    "        seed_embs = self.embeddings(seed_token_ids)  # BatchSize x MaxInLen x EmbSize\n",
    "        pos_codes = make_positional_encoding(max_in_length,\n",
    "                                             self.embedding_size).unsqueeze(0).to(seed_embs.device)\n",
    "        seed_embs = seed_embs + pos_codes\n",
    "        seed_embs = self.emb_dropout(seed_embs)\n",
    "\n",
    "        # BatchSize x TargetLen x EmbSize\n",
    "        target_features = seed_embs\n",
    "        target_features = self.backbone(seed_embs,\n",
    "                                        mask=dependency_mask,\n",
    "                                        src_key_padding_mask=seed_padding_mask)\n",
    "        logits = self.out(target_features)  # BatchSize x TargetLen x VocabSize\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-surgery",
   "metadata": {},
   "source": [
    "## Утилиты для обучения - функция потерь и расписание изменения длины градиентного шага"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_cross_entropy(pred, target):\n",
    "    \"\"\"\n",
    "    pred - BatchSize x TargetLen x VocabSize\n",
    "    target - BatchSize x TargetLen\n",
    "    \"\"\"\n",
    "    pred_flat = pred.view(-1, pred.shape[-1])  # BatchSize*TargetLen x VocabSize\n",
    "    target_flat = target.view(-1)  # BatchSize*TargetLen\n",
    "    return F.cross_entropy(pred_flat, target_flat, ignore_index=0)\n",
    "\n",
    "\n",
    "def lr_scheduler(optimizer):\n",
    "    return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                      patience=20,\n",
    "                                                      factor=0.5,\n",
    "                                                      verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-drove",
   "metadata": {},
   "source": [
    "## Реализация Transformer из PyTorch 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchFirstTransformerEncoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.impl = nn.TransformerEncoder(*args, **kwargs)\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def forward(self, src, *args, **kwargs):\n",
    "        src = src.transpose(0, 1).contiguous()  # MaxInLen  x BatchSize x EmbSize\n",
    "        result = self.impl(src, *args, **kwargs)  # TargetLen x BatchSize x EmbSize\n",
    "        result = result.transpose(0, 1).contiguous()  # BatchSize x TargetLen x EmbSize\n",
    "        return result\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for param in self.impl.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_transf_model = LanguageModel(tokenizer.vocab_size(),\n",
    "                                   256,\n",
    "                                   BatchFirstTransformerEncoder(\n",
    "                                       nn.TransformerEncoderLayer(\n",
    "                                           d_model=256,\n",
    "                                           nhead=16,\n",
    "                                           dim_feedforward=512,\n",
    "                                           dropout=0.1),\n",
    "                                       num_layers=3),\n",
    "                                   emb_dropout=0.1)\n",
    "print('Количество параметров', get_params_number(torch_transf_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "(best_val_loss,\n",
    " best_torch_transf_model) = train_eval_loop(torch_transf_model,\n",
    "                                            train_dataset,\n",
    "                                            test_dataset,\n",
    "                                            lm_cross_entropy,\n",
    "                                            lr=2e-3,\n",
    "                                            epoch_n=2000,\n",
    "                                            batch_size=512,\n",
    "                                            device='cuda',\n",
    "                                            early_stopping_patience=50,\n",
    "                                            max_batches_per_epoch_train=1000,\n",
    "                                            max_batches_per_epoch_val=1000,\n",
    "                                            lr_scheduler_ctor=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_torch_transf_model.state_dict(), './models/war_and_peace_torch_transf_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_transf_model.load_state_dict(torch.load('./models/war_and_peace_torch_transf_best.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-found",
   "metadata": {},
   "source": [
    "## Генерация текста с помощью языковой модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-brother",
   "metadata": {},
   "source": [
    "### Жадная генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_generator = GreedyGenerator(torch_transf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(greedy_generator('сказала княжна, оглядывая Бона'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(greedy_generator('смеялась княжна, оглядывая Наполе'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(greedy_generator('сказала княжна, оглядывая Кутуз'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-advocate",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(greedy_generator('сказал Кутузов, оглядывая Наполеона'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-lebanon",
   "metadata": {},
   "source": [
    "## Генерация с помощью лучевого поиска - Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_generator = BeamGenerator(torch_transf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "beam_gen_variants = beam_generator('сказала княжна, оглядывая Наполе',\n",
    "                                   beamsize=5,\n",
    "                                   return_hypotheses_n=5)\n",
    "\n",
    "for score, pred_txt in beam_gen_variants:\n",
    "    print('****')\n",
    "    print(score)\n",
    "    print(pred_txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "beam_gen_variants = beam_generator('сказала княжна, оглядывая Наполе',\n",
    "                                   beamsize=20,\n",
    "                                   return_hypotheses_n=20)\n",
    "\n",
    "for score, pred_txt in beam_gen_variants:\n",
    "    print('****')\n",
    "    print(score)\n",
    "    print(pred_txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "beam_gen_variants = beam_generator('сказала княжна, оглядывая Наполе',\n",
    "                                   beamsize=100,\n",
    "                                   return_hypotheses_n=20)\n",
    "\n",
    "for score, pred_txt in beam_gen_variants:\n",
    "    print('****')\n",
    "    print(score)\n",
    "    print(pred_txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-hobby",
   "metadata": {},
   "source": [
    "## Собственная реализация MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_multihead_attention(queries, keys, values,\n",
    "                           keys_padding_mask, dependency_mask,\n",
    "                           is_training,\n",
    "                           weights_dropout):\n",
    "    \"\"\"\n",
    "    queries - BatchSize x ValuesLen x HeadN x KeySize\n",
    "    keys - BatchSize x KeysLen x HeadN x KeySize\n",
    "    values - BatchSize x KeysLen x HeadN x ValueSize\n",
    "    keys_padding_mask - BatchSize x KeysLen\n",
    "    dependency_mask - ValuesLen x KeysLen\n",
    "    is_training - bool\n",
    "    weights_dropout - float\n",
    "    \n",
    "    result - tuple of two:\n",
    "        - BatchSize x ValuesLen x HeadN x ValueSize - resulting features\n",
    "        - BatchSize x ValuesLen x KeysLen x HeadN - attention map\n",
    "    \"\"\"\n",
    "\n",
    "    # BatchSize x ValuesLen x KeysLen x HeadN\n",
    "    relevances = torch.einsum('bvhs,bkhs->bvkh', (queries, keys))\n",
    "    \n",
    "    # замаскировать элементы, выходящие за длины последовательностей ключей\n",
    "    padding_mask_expanded = keys_padding_mask[:, None, :, None].expand_as(relevances)\n",
    "    relevances.masked_fill_(padding_mask_expanded, float('-inf'))\n",
    "    \n",
    "    # замаскировать пары <выходная позиция, входная позиция>\n",
    "    relevances = relevances + dependency_mask[None, :, :, None].expand_as(relevances)\n",
    "    \n",
    "    normed_rels = F.softmax(relevances, dim=2)    \n",
    "    normed_rels = F.dropout(normed_rels, weights_dropout, is_training)\n",
    "    \n",
    "    # BatchSize x ValuesLen x KeysLen x HeadN x 1\n",
    "    normed_rels_expanded = normed_rels.unsqueeze(-1)\n",
    "    \n",
    "    # BatchSize x 1 x KeysLen x HeadN x ValueSize\n",
    "    values_expanded = values.unsqueeze(1)\n",
    "    \n",
    "    # BatchSize x ValuesLen x KeysLen x HeadN x ValueSize\n",
    "    weighted_values = normed_rels_expanded * values_expanded\n",
    "    result = weighted_values.sum(2)  # BatchSize x ValuesLen x HeadN x ValueSize\n",
    "    \n",
    "    return result, normed_rels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-nepal",
   "metadata": {},
   "source": [
    "## Self-Attention - это Attention, в котором ключи, значения и запросы вычисляются из элементов одной и той же последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, model_size, n_heads, dropout=0):\n",
    "        super().__init__()\n",
    "        assert model_size % n_heads == 0, 'Размерность модели должна делиться нацело на количество голов'\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.queries_proj = nn.Linear(model_size, model_size)\n",
    "        self.keys_proj = nn.Linear(model_size, model_size)\n",
    "        self.values_proj = nn.Linear(model_size, model_size)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.last_attention_map = None\n",
    "    \n",
    "    def forward(self, sequence, padding_mask, dependency_mask):\n",
    "        \"\"\"\n",
    "        sequence - BatchSize x Len x ModelSize\n",
    "        padding_mask - BatchSize x Len\n",
    "        dependency_mask - Len x Len\n",
    "        \n",
    "        result - BatchSize x Len x ModelSize\n",
    "        \"\"\"\n",
    "        batch_size, max_len, model_size = sequence.shape\n",
    "        \n",
    "        queries_flat = self.queries_proj(sequence)  # BatchSize x Len x ModelSize\n",
    "        queries = queries_flat.view(batch_size, max_len, self.n_heads, -1)\n",
    "        \n",
    "        keys_flat = self.keys_proj(sequence)  # BatchSize x Len x ModelSize\n",
    "        keys = keys_flat.view(batch_size, max_len, self.n_heads, -1)\n",
    "        \n",
    "        values_flat = self.values_proj(sequence)  # BatchSize x Len x ModelSize\n",
    "        values = values_flat.view(batch_size, max_len, self.n_heads, -1)\n",
    "        \n",
    "        # BatchSize x Len x HeadsN x ValueSize\n",
    "        result, att_map = my_multihead_attention(queries, keys, values,\n",
    "                                                 padding_mask, dependency_mask,\n",
    "                                                 self.training, self.dropout)\n",
    "        result_flat = result.view(batch_size, max_len, model_size)\n",
    "        \n",
    "        self.last_attention_map = att_map.detach()\n",
    "\n",
    "        return result_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-shepherd",
   "metadata": {},
   "source": [
    "## Один слой трансформера - Self-Attention, Feed-Forward, skip-connections, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_size, n_heads, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attention = MyMultiheadSelfAttention(model_size,\n",
    "                                                       n_heads,\n",
    "                                                       dropout=dropout)\n",
    "        self.first_dropout = nn.Dropout(dropout)\n",
    "        self.first_norm = nn.LayerNorm(model_size)\n",
    "        \n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(model_size, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, model_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.second_norm = nn.LayerNorm(model_size)\n",
    "    \n",
    "    def forward(self, sequence, padding_mask, dependency_mask):\n",
    "        att_features = self.self_attention(sequence, padding_mask, dependency_mask)\n",
    "\n",
    "        sequence = sequence + self.first_dropout(att_features)\n",
    "        sequence = self.first_norm(sequence)\n",
    "        \n",
    "        sequence = sequence + self.feedforward(sequence)\n",
    "        sequence = self.second_norm(sequence)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-mapping",
   "metadata": {},
   "source": [
    "## Энкодер Трансформера - стопка из нескольких слоёв"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransformerEncoder(nn.Module):\n",
    "    def __init__(self, n_layers, **layer_kwargs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MyTransformerEncoderLayer(**layer_kwargs)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, sequence, mask, src_key_padding_mask):\n",
    "        for layer in self.layers:\n",
    "            sequence = layer(sequence, src_key_padding_mask, mask)\n",
    "        return sequence\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-vatican",
   "metadata": {},
   "source": [
    "## Попробуем обучить языковую модель с нашим Трансформером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_transf_model = LanguageModel(tokenizer.vocab_size(),\n",
    "                                256,\n",
    "                                MyTransformerEncoder(\n",
    "                                    n_layers=3,\n",
    "                                    model_size=256,\n",
    "                                    n_heads=16,\n",
    "                                    dim_feedforward=512,\n",
    "                                    dropout=0.1),\n",
    "                                emb_dropout=0.1)\n",
    "print('Количество параметров', get_params_number(my_transf_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "(best_val_loss,\n",
    " best_my_transf_model) = train_eval_loop(my_transf_model,\n",
    "                                         train_dataset,\n",
    "                                         test_dataset,\n",
    "                                         lm_cross_entropy,\n",
    "                                         lr=2e-3,\n",
    "                                         epoch_n=2000,\n",
    "                                         batch_size=512,\n",
    "                                         device='cuda',\n",
    "                                         early_stopping_patience=50,\n",
    "                                         max_batches_per_epoch_train=1000,\n",
    "                                         max_batches_per_epoch_val=1000,\n",
    "                                         lr_scheduler_ctor=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_my_transf_model.state_dict(), './models/war_and_peace_my_transf_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transf_model.load_state_dict(torch.load('./models/war_and_peace_my_transf_best.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-northwest",
   "metadata": {},
   "source": [
    "## Наша реализация - жадная генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_greedy_generator = GreedyGenerator(my_transf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_greedy_generator('сказала княжна, оглядывая Андре')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-saskatchewan",
   "metadata": {},
   "source": [
    "## Визуализация карт внимания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_attention_maps(model, input_string, tokenizer, device='cuda', max_heads=2, figsize=(16, 10)):\n",
    "    device = torch.device(device)\n",
    "\n",
    "    token_ids = tokenizer.encode([input_string])[0]\n",
    "\n",
    "    token_strs = [tokenizer.id_to_subword(i) for i in token_ids]\n",
    "    in_len = len(token_ids)\n",
    "    ticks = np.arange(0, in_len)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    in_batch = torch.tensor(token_ids).unsqueeze(0).to(device)\n",
    "    model(in_batch)\n",
    "\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, MyMultiheadSelfAttention):\n",
    "            cur_last_attention_map = module.last_attention_map[0].cpu().numpy()\n",
    "            n_heads = cur_last_attention_map.shape[-1]\n",
    "            n_heads_to_vis = min(n_heads, max_heads)\n",
    "\n",
    "            fig, axes = plt.subplots(1, n_heads_to_vis)\n",
    "            fig.set_size_inches(figsize)\n",
    "            for head_i in range(n_heads_to_vis):\n",
    "                ax = axes[head_i]\n",
    "                ax.imshow(cur_last_attention_map[..., head_i])\n",
    "\n",
    "                ax.set_yticks(ticks)\n",
    "                ax.set_ylim(bottom=in_len - 0.5, top=-0.5)\n",
    "                ax.set_yticklabels(token_strs)\n",
    "\n",
    "                ax.set_xticks(ticks)\n",
    "                ax.set_xticklabels(token_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_maps(my_transf_model, 'сказал Кутузов, оглядывая Бонапарта', tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
